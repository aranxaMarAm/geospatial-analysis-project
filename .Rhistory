#S1: unemployment rate in percentage
qtm(joined_sf, fill="iica", fill.scale=tm_scale(values="viridis", n=10))
# first we again define the neighbors
# in this case, we consider direct neighbors
nb <- poly2nb(joined_sf, queen = TRUE)
# the we create weights for the neighbors with style: row standardized
nbw <- nb2listw(nb, style = "W", zero.policy = T)
# we set our hypothesis to: alternative to "greater", which means we expect positive autocorrelation
gmoran <- moran.test(joined_sf$iica, nbw,
alternative = "greater")
gmoran
library(sf)
# Identify invalid geometries
invalid_rows <- which(!st_is_valid(joined_sf))
print(invalid_rows)
# Visualize one of them (optional)
plot(st_geometry(joined_sf[invalid_rows[1], ]))
# first we again define the neighbors
# in this case, we consider direct neighbors
nb <- poly2nb(joined_sf, queen = TRUE)
# the we create weights for the neighbors with style: row standardized
nbw <- nb2listw(nb, style = "W", zero.policy = T)
# we set our hypothesis to: alternative to "greater", which means we expect positive autocorrelation
gmoran <- moran.test(joined_sf$iica, nbw,
alternative = "greater")
gmoran
library(sf)
# Identify invalid geometries
invalid_rows <- which(!st_is_valid(joined_sf))
print(invalid_rows)
# Visualize one of them (optional)
plot(st_geometry(joined_sf[invalid_rows[1], ]))
# Attempt to fix geometries
joined_fixed <- st_make_valid(joined_sf)
# first we again define the neighbors
# in this case, we consider direct neighbors
nb <- poly2nb(joined_fixed, queen = TRUE)
# the we create weights for the neighbors with style: row standardized
nbw <- nb2listw(nb, style = "W", zero.policy = T)
# we set our hypothesis to: alternative to "greater", which means we expect positive autocorrelation
gmoran <- moran.test(joined_sf$iica, nbw,
alternative = "greater")
gmoran
library(sf)
# Identify invalid geometries
invalid_rows <- which(!st_is_valid(joined_sf))
print(invalid_rows)
# Visualize one of them (optional)
plot(st_geometry(joined_sf[invalid_rows[1], ]))
# Attempt to fix geometries
joined_fixed <- st_make_valid(joined_sf)
library(spdep)
nb <- poly2nb(joined_fixed, queen = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Load Packages
library(sf) |> suppressMessages() # for spatial vector data
library(dplyr) |> suppressMessages()
library(tidyr)
library(stringr)
library (terra) # for spatial data analysis with vector and raster data
library(tmap) # for static and interactive maps
library(spdep) # for spatial dependency (session 6)
library(leaflet) # for interactive maps
library(units) |> suppressMessages() # for measurement units in R vectors, matrices and arrays automatic propagation, conversion, derivation and simplification of units
library(giscoR)
library(arrow)
library(jsonlite)
library(geojsonsf)
library(geojsonio)
library(spatialreg)
library(gridExtra)
if (!require("tidycensus", quietly = TRUE)) install.packages("tidycensus")
install.packages("sf", type = "source")
knitr::opts_chunk$set(echo = TRUE)
# Load Packages
library(sf) |> suppressMessages() # for spatial vector data
library(dplyr) |> suppressMessages()
library(tidyr)
library(stringr)
library (terra) # for spatial data analysis with vector and raster data
library(tmap) # for static and interactive maps
library(spdep) # for spatial dependency (session 6)
library(leaflet) # for interactive maps
library(units) |> suppressMessages() # for measurement units in R vectors, matrices and arrays automatic propagation, conversion, derivation and simplification of units
library(giscoR)
library(arrow)
library(jsonlite)
library(geojsonsf)
library(geojsonio)
library(spatialreg)
library(gridExtra)
if (!require("tidycensus", quietly = TRUE)) install.packages("tidycensus")
# Load data
zip_filename <- "final_df.zip"
csv_filename <- "../final_df.csv"
# Temporary directory
temp_dir <- tempdir()
# Extract the specific CSV file to temp dir
unzip(zipfile = zip_filename, files = csv_filename, exdir = temp_dir)
# Extracted file will retain directory structure, verify exact path:
extracted_file_path <- file.path(temp_dir, csv_filename)
# Normalize path (since "../" could cause issues)
extracted_file_path <- normalizePath(extracted_file_path, mustWork = FALSE)
# Check if extraction succeeded:
if (!file.exists(extracted_file_path)) {
stop("Extraction failed. Check filenames and paths.")
}
# Read the CSV into a dataframe
final_df <- read.csv(extracted_file_path)
# Clean up by deleting the file
file.remove(extracted_file_path)
# Read GeoJSON
muni_sf <- geojson_sf("colombia_municipios_poblacion.json")
head(muni_sf)
## Assigning manually the Coordinate Reference System (CRS)
st_crs(muni_sf) <- 4326
# Check number of geometries in original shapefile
nrow(muni_sf)
# Check number of records in final_df
nrow(final_df)
# Check join key overlap
length(intersect(muni_sf$MPIO_CDPMP, final_df$codmpio))
# Convert both codes to character to ensure matching works
final_df <- final_df %>%
mutate(codmpio = as.character(codmpio))
muni_sf <- muni_sf %>%
mutate(MPIO_CCDGO = as.character(MPIO_CDPMP))
final_22 <- final_df %>%
filter(year == 2022)
muni_sf <- muni_sf %>%
mutate(MPIO_CCDGO = str_pad(as.character(MPIO_CDPMP), width = 5, pad = "0"))
final_22 <- final_22 %>%
mutate(codmpio = str_pad(as.character(codmpio), width = 5, pad = "0"))
# drop all areas with NA values
joined_sf <- joined_sf |> filter(!is.na(iica))
#S1: unemployment rate in percentage
qtm(joined_sf, fill="iica", fill.scale=tm_scale(values="viridis", n=10))
library(sf)
sf_use_s2(FALSE)
# Load data
zip_filename <- "final_df.zip"
csv_filename <- "../final_df.csv"
# Temporary directory
temp_dir <- tempdir()
# Extract the specific CSV file to temp dir
unzip(zipfile = zip_filename, files = csv_filename, exdir = temp_dir)
# Extracted file will retain directory structure, verify exact path:
extracted_file_path <- file.path(temp_dir, csv_filename)
# Normalize path (since "../" could cause issues)
extracted_file_path <- normalizePath(extracted_file_path, mustWork = FALSE)
# Check if extraction succeeded:
if (!file.exists(extracted_file_path)) {
stop("Extraction failed. Check filenames and paths.")
}
# Read the CSV into a dataframe
final_df <- read.csv(extracted_file_path)
# Clean up by deleting the file
file.remove(extracted_file_path)
head(final_df)
# Read GeoJSON
muni_sf <- geojson_sf("colombia_municipios_poblacion.json")
head(muni_sf)
## Assigning manually the Coordinate Reference System (CRS)
st_crs(muni_sf) <- 4326
# Check number of geometries in original shapefile
nrow(muni_sf)
# Check number of records in final_df
nrow(final_df)
# Check join key overlap
length(intersect(muni_sf$MPIO_CDPMP, final_df$codmpio))
# Convert both codes to character to ensure matching works
final_df <- final_df %>%
mutate(codmpio = as.character(codmpio))
muni_sf <- muni_sf %>%
mutate(MPIO_CCDGO = as.character(MPIO_CDPMP))
final_22 <- final_df %>%
filter(year == 2022)
muni_sf <- muni_sf %>%
mutate(MPIO_CCDGO = str_pad(as.character(MPIO_CDPMP), width = 5, pad = "0"))
final_22 <- final_22 %>%
mutate(codmpio = str_pad(as.character(codmpio), width = 5, pad = "0"))
joined_sf <- left_join(muni_sf, final_22, by = c("MPIO_CDPMP" = "codmpio"))
# drop all areas with NA values
joined_sf <- joined_sf |> filter(!is.na(iica))
#S1: unemployment rate in percentage
qtm(joined_sf, fill="iica", fill.scale=tm_scale(values="viridis", n=10))
# first we again define the neighbors
# in this case, we consider direct neighbors
nb <- poly2nb(joined_sf, queen = TRUE)
# the we create weights for the neighbors with style: row standardized
nbw <- nb2listw(nb, style = "W", zero.policy = T)
# we set our hypothesis to: alternative to "greater", which means we expect positive autocorrelation
gmoran <- moran.test(joined_sf$iica, nbw,
alternative = "greater")
gmoran
mp <- moran.plot(joined_sf$iica, nbw, labels=F)
mp
# spatially lagged values: mp$wx
lmoran <- localmoran(joined_sf$iica, nbw, alternative = "two.sided")
head(lmoran)
joined_sf$lmI <- lmoran[, "Ii"] # local Moran's I
# p-values corresponding to alternative greater
joined_sf$lmp <- lmoran[, "Pr(z != E(Ii))"]
joined_sf$lmI_sign <- joined_sf$lmI
#joined_sf[joined_sf$lmp >= 0.05, "lmI_sign"] <- NA
qtm(joined_sf, fill="lmI_sign")
tm_shape(joined_sf) +
tm_polygons(
fill = "lmI_sign",
fill.legend = tm_legend(title = "spatial autocorrelation"),
fill.scale=tm_scale(
breaks = c(min(joined_sf$lmI_sign, na.rm=T), 0, max(joined_sf$lmI_sign, na.rm=T)),
labels = c("Negative SAC","Positive SAC"),
textNA = "not significant")
)
# scale such that mean is 0
#mp <- moran.plot(as.vector(scale(la_income$estimate)), nbw)
# get quadrant information
joined_sf$quadr <- attributes(lmoran)$quadr$mean
levels(joined_sf$quadr) <- c(levels(joined_sf$quadr), "non-significant")
joined_sf[(joined_sf$lmp >= 0.05) & !is.na(joined_sf$lmp), "quadr"] <- "non-significant"
tm_shape(joined_sf) +
tm_polygons(
fill = "quadr",
fill.scale=tm_scale(values = c("blue", "lightpink", "skyblue2", "red", "white"))
)
hist(joined_sf$iica, nclass=50)
hist(log(joined_sf$iica), nclass=50)
# Filter rows without NAs (just in case)
joined_clean <- joined_sf %>%
filter(!is.na(iica), iica > 0) %>%
filter(!is.na(gdp_pc)) %>%
filter(!is.na(H_coca)) %>%
filter(!is.na(fisc_perf)) %>%
filter(!is.na(e_desplaza))
joined_clean2 <- joined_clean |>
na.omit()
# this is our initial regression model
formula <- formula(log(iica) ~ gdp_pc + H_coca + fisc_perf + e_desplaza)
model1 <- lm(formula = formula, data = joined_clean, na.action = na.omit)
summary(model1)
# Filter rows without NAs (just in case)
joined_clean <- joined_sf %>%
filter(!is.na(iica), iica > 0) %>%
filter(!is.na(gdp_pc)) %>%
filter(!is.na(H_coca)) %>%
filter(!is.na(fisc_perf)) %>%
filter(!is.na(e_desplaza))
joined_clean2 <- joined_clean |>
na.omit()
# this is our initial regression model
formula <- formula(log(iica) ~ gdp_pc + H_coca + fisc_perf + e_desplaza)
model1 <- lm(formula = formula, data = joined_clean, na.action = na.omit)
summary(model1)
# create list of neighbors
wts <- joined_clean |>
poly2nb() |>
nb2listw(style = "W", zero.policy = T)
# use Morans I test if there is still autocorrelation in our residuals
moran.test(model1$residuals, wts)
vars_used <- all.vars(formula(model1))  # Includes dependent + independent variables
summary(joined_clean[, vars_used])      # Check for NAs, negative values, etc.
class(joined_clean)
# Should include "sf"
library(spatialreg)
formula <- formula(log(iica) ~ gdp_pc + H_coca + fisc_perf + e_desplaza)
# SLX model (includes spatial lags of X variables only)
slx_model <- lmSLX(formula, data = joined_clean, listw = wts, zero.policy = TRUE)
summary(slx_model)
library(ggplot2)
residuals_model1 <- residuals(model1)
lagged_residuals_model1 <- lag.listw(wts, residuals_model1)
res <- data.frame("lagged_residuals"=lagged_residuals_model1, "residuals"=residuals_model1)
ggplot(res, aes(x = residuals, y = lagged_residuals)) +
theme_minimal() +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "red")
summary(impacts(slx_model))
moran.test(slx_model$residuals, wts)
# spatial lag models
lag_model <- lagsarlm(
formula = formula,
data = df,
listw = wts, # taking the same weights as before
zero.policy = T
)
# spatial lag models
lag_model <- lagsarlm(
formula = formula,
data = joined_clean,
listw = wts, # taking the same weights as before
zero.policy = T
)
summary(lag_model, Nagelkerke = TRUE)
imp <- impacts(lag_model, listw = wts, R=100) # compute impacts
imp
#To print the p values
summary(imp, zstats=T)$pzmat
moran.test(lag_model$residuals, wts)
error_model <- errorsarlm(
formula = formula,
data = joined_clean,
listw = wts
)
moran.test(lag_model$residuals, wts)
error_model <- errorsarlm(
formula = formula,
data = joined_clean,
listw = wts
)
error_model <- errorsarlm(
formula = formula,
data = joined_clean,
listw = wts,
zero.policy = TRUE
)
summary(error_model, Nagelkerke = TRUE)
moran.test(error_model$residuals, wts)
lm.LMtests(
model1,
wts,
test = c("LMerr", "LMlag")
# LM error, LM lag
)
lm.LMtests(
model1,
wts,
test = c("RLMerr", "RLMlag")
# robust LM error, robust LM lag
)
AIC(model1, lag_model, error_model)
knitr::opts_chunk$set(echo = TRUE)
library(sf) |> suppressMessages()
library(dplyr) |> suppressMessages()
library(tidyr)
library(stringr)
library (terra)
library(tmap)
library(spdep)
library(leaflet)
library(units) |> suppressMessages()
library(giscoR)
library(arrow)
library(jsonlite)
library(geojsonsf)
library(geojsonio)
library(spatialreg)
library(gridExtra)
library(sf)
sf_use_s2(FALSE)
# Load data
zip_filename <- "final_df.zip"
csv_filename <- "../final_df.csv"
# Temporary directory
temp_dir <- tempdir()
# Extract the specific CSV file to temp dir
unzip(zipfile = zip_filename, files = csv_filename, exdir = temp_dir)
# Extracted file will retain directory structure, verify exact path:
extracted_file_path <- file.path(temp_dir, csv_filename)
# Normalize path (since "../" could cause issues)
extracted_file_path <- normalizePath(extracted_file_path, mustWork = FALSE)
# Check if extraction succeeded:
if (!file.exists(extracted_file_path)) {
stop("Extraction failed. Check filenames and paths.")
}
# Read the CSV into a dataframe
final_df <- read.csv(extracted_file_path)
# Clean up by deleting the file
file.remove(extracted_file_path)
head(final_df)
# Read GeoJSON
muni_sf <- geojson_sf("colombia_municipios_poblacion.json")
head(muni_sf)
## Assigning manually the Coordinate Reference System (CRS)
st_crs(muni_sf) <- 4326
# Check number of geometries in original shapefile
nrow(muni_sf)
# Check number of records in final_df
nrow(final_df)
# Check join key overlap
length(intersect(muni_sf$MPIO_CDPMP, final_df$codmpio))
# Convert both codes to character to ensure matching works
final_df <- final_df %>%
mutate(codmpio = as.character(codmpio))
muni_sf <- muni_sf %>%
mutate(MPIO_CCDGO = as.character(MPIO_CDPMP))
final_22 <- final_df %>%
filter(year == 2022)
muni_sf <- muni_sf %>%
mutate(MPIO_CCDGO = str_pad(as.character(MPIO_CDPMP), width = 5, pad = "0"))
final_22 <- final_22 %>%
mutate(codmpio = str_pad(as.character(codmpio), width = 5, pad = "0"))
joined_sf <- left_join(muni_sf, final_22, by = c("MPIO_CDPMP" = "codmpio"))
# drop all areas with NA values
joined_sf <- joined_sf |> filter(!is.na(iica))
#S1: unemployment rate in percentage
qtm(joined_sf, fill="iica", fill.scale=tm_scale(values="viridis", n=10))
# first we again define the neighbors
# in this case, we consider direct neighbors
nb <- poly2nb(joined_sf, queen = TRUE)
# the we create weights for the neighbors with style: row standardized
nbw <- nb2listw(nb, style = "W", zero.policy = T)
# we set our hypothesis to: alternative to "greater", which means we expect positive autocorrelation
gmoran <- moran.test(joined_sf$iica, nbw,
alternative = "greater")
gmoran
mp <- moran.plot(joined_sf$iica, nbw, labels=F)
mp
# spatially lagged values: mp$wx
lmoran <- localmoran(joined_sf$iica, nbw, alternative = "two.sided")
head(lmoran)
joined_sf$lmI <- lmoran[, "Ii"] # local Moran's I
# p-values corresponding to alternative greater
joined_sf$lmp <- lmoran[, "Pr(z != E(Ii))"]
joined_sf$lmI_sign <- joined_sf$lmI
#joined_sf[joined_sf$lmp >= 0.05, "lmI_sign"] <- NA
qtm(joined_sf, fill="lmI_sign")
tm_shape(joined_sf) +
tm_polygons(
fill = "lmI_sign",
fill.legend = tm_legend(title = "spatial autocorrelation"),
fill.scale=tm_scale(
breaks = c(min(joined_sf$lmI_sign, na.rm=T), 0, max(joined_sf$lmI_sign, na.rm=T)),
labels = c("Negative SAC","Positive SAC"),
textNA = "not significant")
)
# scale such that mean is 0
#mp <- moran.plot(as.vector(scale(la_income$estimate)), nbw)
# get quadrant information
joined_sf$quadr <- attributes(lmoran)$quadr$mean
levels(joined_sf$quadr) <- c(levels(joined_sf$quadr), "non-significant")
joined_sf[(joined_sf$lmp >= 0.05) & !is.na(joined_sf$lmp), "quadr"] <- "non-significant"
tm_shape(joined_sf) +
tm_polygons(
fill = "quadr",
fill.scale=tm_scale(values = c("blue", "lightpink", "skyblue2", "red", "white"))
)
hist(joined_sf$iica, nclass=50)
hist(log(joined_sf$iica), nclass=50)
# Filter rows without NAs (just in case)
joined_clean <- joined_sf %>%
filter(!is.na(iica), iica > 0) %>%
filter(!is.na(gdp_pc)) %>%
filter(!is.na(H_coca)) %>%
filter(!is.na(fisc_perf)) %>%
filter(!is.na(e_desplaza))
joined_clean2 <- joined_clean |>
na.omit()
# this is our initial regression model
formula <- formula(log(iica) ~ gdp_pc + H_coca + fisc_perf + e_desplaza)
model1 <- lm(formula = formula, data = joined_clean, na.action = na.omit)
summary(model1)
# create list of neighbors
wts <- joined_clean |>
poly2nb() |>
nb2listw(style = "W", zero.policy = T)
# use Morans I test if there is still autocorrelation in our residuals
moran.test(model1$residuals, wts)
library(ggplot2)
residuals_model1 <- residuals(model1)
lagged_residuals_model1 <- lag.listw(wts, residuals_model1)
res <- data.frame("lagged_residuals"=lagged_residuals_model1, "residuals"=residuals_model1)
ggplot(res, aes(x = residuals, y = lagged_residuals)) +
theme_minimal() +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "red")
vars_used <- all.vars(formula(model1))  # Includes dependent + independent variables
summary(joined_clean[, vars_used])      # Check for NAs, negative values, etc.
class(joined_clean)
# Should include "sf"
library(spatialreg)
formula <- formula(log(iica) ~ gdp_pc + H_coca + fisc_perf + e_desplaza)
# SLX model (includes spatial lags of X variables only)
slx_model <- lmSLX(formula, data = joined_clean, listw = wts, zero.policy = TRUE)
summary(slx_model)
summary(impacts(slx_model))
moran.test(slx_model$residuals, wts)
# spatial lag models
lag_model <- lagsarlm(
formula = formula,
data = joined_clean,
listw = wts, # taking the same weights as before
zero.policy = T
)
summary(lag_model, Nagelkerke = TRUE)
imp <- impacts(lag_model, listw = wts, R=100) # compute impacts
imp
#To print the p values
summary(imp, zstats=T)$pzmat
moran.test(lag_model$residuals, wts)
error_model <- errorsarlm(
formula = formula,
data = joined_clean,
listw = wts,
zero.policy = TRUE
)
summary(error_model, Nagelkerke = TRUE)
moran.test(error_model$residuals, wts)
lm.LMtests(
model1,
wts,
test = c("LMerr", "LMlag")
# LM error, LM lag
)
lm.LMtests(
model1,
wts,
test = c("RLMerr", "RLMlag")
# robust LM error, robust LM lag
)
AIC(model1, lag_model, error_model)
moran.test(slx_model$residuals, wts)
