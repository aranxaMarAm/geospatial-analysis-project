---
title: "Geospatial Final Project"
author: "Aranxa Márquez Ampudia & Milton Mier Santander"
date: "2025-05-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Packages to be used

```{r}
# Load Packages

library(sf) |> suppressMessages() # for spatial vector data
library(dplyr) |> suppressMessages()
library(tidyr)
library(stringr)
library (terra) # for spatial data analysis with vector and raster data
library(tmap) # for static and interactive maps
library(spdep) # for spatial dependency (session 6)
library(leaflet) # for interactive maps
library(units) |> suppressMessages() # for measurement units in R vectors, matrices and arrays automatic propagation, conversion, derivation and simplification of units
library(giscoR)
library(arrow)
library(jsonlite)
library(geojsonsf)
library(geojsonio)
library(spatialreg)
library(gridExtra)


if (!require("tidycensus", quietly = TRUE)) install.packages("tidycensus") 

```


# Load indicators data 

```{r}
# Load data

zip_filename <- "final_df.zip"
csv_filename <- "../final_df.csv"

# Temporary directory
temp_dir <- tempdir()

# Extract the specific CSV file to temp dir
unzip(zipfile = zip_filename, files = csv_filename, exdir = temp_dir)

# Extracted file will retain directory structure, verify exact path:
extracted_file_path <- file.path(temp_dir, csv_filename)

# Normalize path (since "../" could cause issues)
extracted_file_path <- normalizePath(extracted_file_path, mustWork = FALSE)

# Check if extraction succeeded:
if (!file.exists(extracted_file_path)) {
    stop("Extraction failed. Check filenames and paths.")
}

# Read the CSV into a dataframe
final_df <- read.csv(extracted_file_path)

# Clean up by deleting the file
file.remove(extracted_file_path)
```

```{r}
head(final_df)
```

# Load geospatial data 

```{r}
# Read GeoJSON
muni_sf <- geojson_sf("colombia_municipios_poblacion.json")
head(muni_sf)
```


```{r}
## Assigning manually the Coordinate Reference System (CRS)
st_crs(muni_sf) <- 4326

# Check number of geometries in original shapefile
nrow(muni_sf)

# Check number of records in final_df
nrow(final_df)

# Check join key overlap
length(intersect(muni_sf$MPIO_CDPMP, final_df$codmpio))
```


# Preprocessing

```{r}
# Convert both codes to character to ensure matching works
final_df <- final_df %>%
  mutate(codmpio = as.character(codmpio))

muni_sf <- muni_sf %>%
  mutate(MPIO_CCDGO = as.character(MPIO_CDPMP))
```


## Filter for 2022

```{r}
final_22 <- final_df %>%
  filter(year == 2022)
```


```{r}
muni_sf <- muni_sf %>%
  mutate(MPIO_CCDGO = str_pad(as.character(MPIO_CDPMP), width = 5, pad = "0"))

final_22 <- final_22 %>%
  mutate(codmpio = str_pad(as.character(codmpio), width = 5, pad = "0"))
```


## Join datasets

```{r}
joined_sf <- left_join(muni_sf, final_22, by = c("MPIO_CDPMP" = "codmpio"))
```


# Spatial Autocorrelation

## 

```{r}

# drop all areas with NA values
joined_sf <- joined_sf |> filter(!is.na(iica))
  
#S1: unemployment rate in percentage
qtm(joined_sf, fill="iica", fill.scale=tm_scale(values="viridis", n=10))

```

## Moran test

```{r}

# first we again define the neighbors
# in this case, we consider direct neigbors
nb <- poly2nb(joined_sf, queen = TRUE)

# the we create weights for the neighbors with style: row standardized
nbw <- nb2listw(nb, style = "W", zero.policy = T)

# we set our hypothesis to: alternative to "greater", which means we expect positive autocorrelation
gmoran <- moran.test(joined_sf$iica, nbw,
                     alternative = "greater")

gmoran

```
```{r}

mp <- moran.plot(joined_clean$iica, nbw, labels=F)
mp
# spatially lagged values: mp$wx

```

## Local Moran's I

Now let's look at the local Moran's I using the `localmoran()` function:

```{r moransI-local}
lmoran <- localmoran(joined_sf$iica, nbw, alternative = "two.sided")
head(lmoran)
```


We now display results for the two-sided test (H1: positive or negative spatial autocorrelation), only considering p values below 0.05 as significant.

```{r moransI-local-map}
joined_sf$lmI <- lmoran[, "Ii"] # local Moran's I
# p-values corresponding to alternative greater
joined_sf$lmp <- lmoran[, "Pr(z != E(Ii))"]
joined_sf$lmI_sign <- joined_sf$lmI
#joined_sf[joined_sf$lmp >= 0.05, "lmI_sign"] <- NA

qtm(joined_sf, fill="lmI_sign")

tm_shape(joined_sf) + 
  tm_polygons(
  fill = "lmI_sign",
  fill.legend = tm_legend(title = "spatial autocorrelation"),
  fill.scale=tm_scale(
    breaks = c(min(joined_sf$lmI_sign, na.rm=T), 0, max(joined_sf$lmI_sign, na.rm=T)),
    labels = c("Negative SAC","Positive SAC"),
    textNA = "not significant")
  )

```


### Clusters

Local Moran’s I allows us to identify clusters of the following types:

-   High-High: areas of high values with neighbors of high values,
-   High-Low: areas of high values with neighbors of low values,
-   Low-High: areas of low values with neighbors of high values,
-   Low-Low: areas of low values with neighbors of low values.

```{r moransI-local-clusters}

# scale such that mean is 0
#mp <- moran.plot(as.vector(scale(la_income$estimate)), nbw)

# get quadrant information
joined_sf$quadr <- attributes(lmoran)$quadr$mean
levels(joined_sf$quadr) <- c(levels(joined_sf$quadr), "non-significant")
joined_sf[(joined_sf$lmp >= 0.05) & !is.na(joined_sf$lmp), "quadr"] <- "non-significant"
```

Plot the significant quadrants:

```{r moransI-local-clusters-plot}
tm_shape(joined_sf) + 
  tm_polygons(
    fill = "quadr",
    fill.scale=tm_scale(values = c("blue", "lightpink", "skyblue2", "red", "white"))
)
```



#




First, we can plot the distribution of the outcome variable.

```{r outcome-variable}
hist(joined_sf$iica, nclass=50)
```

```{r outcome-variable-log}
hist(log(joined_sf$iica), nclass=50)
```



Now we can create an initial (non-spatial) regression model.

```{r regression-model}



# Filter rows without NAs (just in case)
joined_clean <- joined_sf %>%
  filter(!is.na(iica), iica > 0) %>%
  filter(!is.na(gdp_pc)) %>%
  filter(!is.na(H_coca)) %>%
  filter(!is.na(fisc_perf)) %>%
  filter(!is.na(e_desplaza))  

# this is our initial regression model 
formula <- formula(log(iica) ~ gdp_pc + H_coca + fisc_perf + e_desplaza)

model1 <- lm(formula = formula, data = joined_clean, na.action = na.omit)
summary(model1)

```


# Spatial autocorrelation

The former assumption of independence of residuals is commonly violated in models that use
spatial data. This is because models of spatial processes commonly are characterized
by spatial autocorrelation in the error term, meaning that the model’s performance itself
depends on geographic location. We can assess this using techniques learned in the previous
session such as Moran’s I.


```{r autocorrelation}
# create list of neighbors
wts <- joined_clean |>
  poly2nb() |>
  nb2listw(style = "W", zero.policy = T)

# use Morans I test if there is still autocorrelation in our residuals
moran.test(model1$residuals, wts)
```
```{r residual-dist23}
library(ggplot2)
residuals_model1 <- residuals(model1)
lagged_residuals_model1 <- lag.listw(wts, residuals_model1)

res <- data.frame("lagged_residuals"=lagged_residuals_model1, "residuals"=residuals_model1)

ggplot(res, aes(x = residuals, y = lagged_residuals)) +
  theme_minimal() +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "red")
```

The Moran’s I test statistic is modest and positive (0.204) but is statistically significant.

Therefore, we need models that can account for such spatial autocorrelation.

# Spatial regression

There are three simple models that accommodate spatial effects slightly different. The first is the spatially lagged X models:

## Spatially lagged X models 


```{r slx}

# Check for NAs in spatially lagged variables
lag_vars <- create_WX(model.matrix(formula, joined_clean), listw = wts, prefix = "lag.")
summary(as.data.frame(lag_vars))


# spatially lagged X models
slx_model <- lmSLX(formula, data = joined_clean, listw = wts)

summary(slx_model)
```





